{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"transformers>=4.42.0\" \"peft>=0.11.1\" \"trl>=0.9.4\" \"accelerate>=0.34.0\" bitsandbytes datasets evaluate rouge_score einops scipy\n"
      ],
      "metadata": {
        "id": "udF7qYlFd98r",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import tqdm\n",
        "from transformers import (BitsAndBytesConfig,AutoTokenizer,AutoModelForCausalLM,Trainer)\n",
        "from huggingface_hub import interpreter_login\n",
        "from functools import partial\n",
        "from peft import LoraConfig,get_peft_model, prepare_model_for_kbit_training\n",
        "interpreter_login()"
      ],
      "metadata": {
        "id": "clnCa2AD-WmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hugging_face_dataset=\"neil-code/dialogsum-test\"\n",
        "dataset=load_dataset(hugging_face_dataset)"
      ],
      "metadata": {
        "id": "CeJJaKl1BJ7A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "87cco_qQBf20",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "NeIXVcTXBsOL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype=getattr(torch,\"float16\")\n",
        "bnb_config=BitsAndBytesConfig(\n",
        "    Load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype\n",
        ")"
      ],
      "metadata": {
        "id": "eJu23Z0FC9o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"microsoft/phi-2\"\n",
        "device_map={\"\":0}\n",
        "original_model=AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                    quantization_config=bnb_config,\n",
        "                                                    device_map=device_map,\n",
        "                                                    trust_remote_code=True,\n",
        "                                                    use_auth_token=True)"
      ],
      "metadata": {
        "id": "HpmrGZDzCCcB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(model_name,\n",
        "                                        trust_remote_code=True,\n",
        "                                        padding_side=\"left\",\n",
        "                                        add_eos_token=True,\n",
        "                                        add_bos_token=True,\n",
        "                                        use_fast=False)\n",
        "tokenizer.pad_token=tokenizer.eos_token"
      ],
      "metadata": {
        "id": "0n5uQgpzJGIh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(model,tokenizer,input,max_len):\n",
        "  inputs=tokenizer(input,return_tensors=\"pt\")\n",
        "  inputs={k:v.to(\"cuda\") for k,v in inputs.items()}\n",
        "  generated_ids=model.generate(**inputs,max_new_tokens=max_len)\n",
        "  generated_output=tokenizer.batch_decode(generated_ids,skip_special_tokens=True)\n",
        "  return generated_output\n"
      ],
      "metadata": {
        "id": "F58KlxFj18J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from transformers import set_seed\n",
        "seed=42\n",
        "set_seed(seed)\n",
        "index=10\n",
        "prompt=dataset['test'][index]['dialogue']\n",
        "summary=dataset['test'][index]['summary']\n",
        "formated_prompt=f\"Instruct:Summarize the following Conversation.\\n{prompt}\\nOutput:\\n\"\n",
        "\n",
        "res=gen(original_model,tokenizer,formated_prompt,200)\n",
        "output = res[0].split('Output:\\n')[1]\n",
        "\n",
        "dash_line='-'*100\n",
        "\n",
        "print(f\"INPUT PROMPT:\\n{formated_prompt}\")\n",
        "print(dash_line)\n",
        "print(f\"SUMMARY:\\n{summary}\")\n",
        "print(dash_line)\n",
        "print(f\"MODEL GENERATION - ZERO SHOT:\\n{output}\")\n"
      ],
      "metadata": {
        "id": "xZNw-nhOrLHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_format(sample):\n",
        "\n",
        "    INTRO_BLURB = \"Below is an instruction...\"\n",
        "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation\"\n",
        "    RESPONSE_KEY = \"### Output:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    blurb = f\"\\n{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\"\n",
        "\n",
        "    input_context = sample[\"dialogue\"]\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    parts = [blurb, instruction, input_context, response, end]\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "    return sample\n"
      ],
      "metadata": {
        "id": "wj43Egpv1ksD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length(model):\n",
        "  config=model.config\n",
        "  max_length=None\n",
        "\n",
        "  for name in ['n_positions','max_position_embedddings','seq_length']:\n",
        "    max_length=getattr(config,name,None)\n",
        "    if max_length:\n",
        "      print(f\"Found max length :{max_length}\")\n",
        "      break\n",
        "  if not max_length:\n",
        "    max_length=1024\n",
        "    print(f\"Using default max length {max_length}\")\n",
        "  return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch,tokenizer,max_length):\n",
        "\n",
        "  return tokenizer(\n",
        "      batch['text'],\n",
        "      max_length=max_length,\n",
        "      truncation=True\n",
        "  )\n",
        "\n",
        "\n",
        "def preprocess_dataset(tokenizer:AutoTokenizer,max_length:int,seed,dataset):\n",
        "\n",
        "  print(f\"Preprocessing Dataset\")\n",
        "\n",
        "  dataset=dataset.map(create_prompt_format)\n",
        "  preprocessing_fun=partial(preprocess_batch,max_length=max_length,tokenizer=tokenizer)\n",
        "  dataset=dataset.map(preprocessing_fun,batched=True,remove_columns=['id', 'topic', 'dialogue', 'summary'])\n",
        "  dataset=dataset.filter(lambda sample :len(sample[\"input_ids\"])<max_length)\n",
        "\n",
        "  dataset=dataset.shuffle(seed)\n",
        "  return dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "gS2fSMGFgkyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=get_max_length(original_model)\n",
        "\n",
        "train_dataset=preprocess_dataset(tokenizer,max_length,seed,dataset['train'])\n",
        "eval_dataset=preprocess_dataset(tokenizer,max_length,seed,dataset['validation'])"
      ],
      "metadata": {
        "id": "L0kDzRt9rxgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model=prepare_model_for_kbit_training(original_model)\n",
        "\n",
        "config=LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\n",
        "                    \"k_proj\",\n",
        "                    \"v_proj\",\n",
        "                    \"dense\"],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        "\n",
        ")\n",
        "original_model.gradient_checkpointing_enable()\n",
        "peft_model=get_peft_model(original_model,config)"
      ],
      "metadata": {
        "id": "VziBm3thuAuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params}\")\n",
        "    print(f\"All params: {all_param}\")\n",
        "    print(f\"Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
        "    return trainable_params, all_param\n"
      ],
      "metadata": {
        "id": "RKlJuDF1hJqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "Is9bPGXMvU6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "fb2C4Iqmwmtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "output_dir= f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "peft_training_args=TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    warmup_steps=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=1000,\n",
        "    learning_rate=2e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=25,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    do_eval=True,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        "    overwrite_output_dir = 'True',\n",
        "    group_by_length=True,\n",
        ")\n",
        "peft_model.config.use_cache=False\n",
        "peft_trainer=Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=peft_training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "sQzwlG2aHOE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id='microsoft/phi-2'\n",
        "base_model=AutoModelForCausalLM.from_pretrained(base_model_id,\n",
        "                                                device_map='auto',\n",
        "                                                quantization_config=bnb_config,\n",
        "                                                trust_remote_code=True,\n",
        "                                                use_auth_token=True)"
      ],
      "metadata": {
        "id": "S3tY_emTm_BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tokenizer=AutoTokenizer.from_pretrained(base_model_id,add_bos_token=True,trust_remote_code=True,use_fast=False)\n",
        "eval_tokenizer.pad_token=eval_tokenizer.eos_token"
      ],
      "metadata": {
        "id": "cn4SAfUbrjyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/Colab Notebooks/my_folder/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"
      ],
      "metadata": {
        "id": "S4iw126esi2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "seed=42"
      ],
      "metadata": {
        "id": "HxPrqJGyJDfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set_seed(seed)\n",
        "# index=5\n",
        "# dialogue=dataset['test'][index]['dialogue']\n",
        "# summary=dataset['test'][index]['summary']\n",
        "# prompt=f'Instruct: Summarize the following Conversation.\\n {dialogue}\\noutput:\\n'\n",
        "\n",
        "# peft_model_res = gen(ft_model,eval_tokenizer,prompt,100)\n",
        "# print(peft_model_res)\n",
        "# peft_model_output = peft_model_res[0].split('output:\\n')[1]\n",
        "# #print(peft_model_output)\n",
        "# prefix, success, result = peft_model_output.partition('###')\n",
        "\n",
        "# dash_line = '-'.join('' for x in range(100))\n",
        "# print(dash_line)\n",
        "# print(f'INPUT PROMPT:\\n{prompt}')\n",
        "# print(dash_line)\n",
        "# print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "# print(dash_line)\n",
        "# print(f'PEFT MODEL:\\n{prefix}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gwyn0-jK0s6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dialogues=dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries=dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries=[]\n",
        "peft_model_summaries=[]\n",
        "\n",
        "for idx,dialogue in enumerate(dialogues):\n",
        "  human_baseline_text_output=human_baseline_summaries[idx]\n",
        "  prompt=f\"Instruct:Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
        "\n",
        "  original_model_res=gen(base_model,eval_tokenizer,prompt,200)\n",
        "  original_model_text_output=original_model_res[0].split('Output:\\n')[1]\n",
        "\n",
        "  peft_model_res=gen(ft_model,eval_tokenizer,prompt,200)\n",
        "  peft_model_output=peft_model_res[0].split('output:\\n')[1]\n",
        "  print(peft_model_output)\n",
        "  peft_model_text_output,success,result=peft_model_output.partition(\"###\")\n",
        "\n",
        "  original_model_summaries.append(original_model_text_output)\n",
        "  peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "\n",
        "zipped_summaries=list(zip(human_baseline_summaries,original_model_summaries,peft_model_summaries))\n",
        "\n",
        "df=pd.DataFrame(zipped_summaries,columns=['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
        "\n"
      ],
      "metadata": {
        "id": "lvAyMJqC9FRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7Frd5F8Fjjln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "rouge=evaluate.load('rouge')\n",
        "original_model_results=rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "peft_model_results=rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)\n",
        "\n",
        "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "improvement=(np.array(list(peft_model_results.values()))-np.array(list(original_model_results.values())))\n",
        "for key,value in zip(peft_model_results.keys(),improvement):\n",
        "  print(f'{key}: {value*100:2f}%')"
      ],
      "metadata": {
        "id": "LX95BA58_El1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLdBu49H_GKu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}